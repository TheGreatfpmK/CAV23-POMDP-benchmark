# Search and Explore: Symbiotic Policy Synthesis in POMDPs 
# CAV'23 Benchmarks

---

## Benchmark script

Place all of the files of this repository to folder called `experiments` in the root folder of PAYNT

Run the benchmarks using:
>./experiments/benchmark.sh

- `-a` use this flag to run experiments both from the paper and the appendix (without this flag only the main experiments run)
- `-x` use this flag to only run experiments from the appendix
- `-o` with this flag overwriting of already existing logs is allowed (good if you want to rerun all the experiments), overwriting is turned off by default so if you only want to run experiments for certain values remove the logs associated with the values and rerun this script without -o flag
- `-e` allows export of the FSCs found by SAYNT, THIS OPTION REQUIRES FEW GIGABYTES OF DISK SPACE TO STORE ALL THE FSCs AND MIGHT INFLUENCE THE SPEED OF THE METHODS FOR LARGE MODELS!

The script automatically runs all the experiments and creates PDFs containing tables/graphs. All the PDFs (and the source .tex files) will be located in `experiments/results` folder. If you used the -e flag you can find the exported FSCs alongside the log files.

The logs, from which the tables are created, are saved into corresponding folders e.g. experiments to obtain information about models (table 2 of our paper) will store the logs to models-info folder and so on.

We also provide our logs in the `experiments/original-results`, in case something goes wrong.

The script is pretty fail-safe meaning that it will finish and generate all the output even if some experiment fails (it will notify you with ERROR message on stdout). So if some of the generated tables are missing rows, check the output of the benchmark script to see what experiments failed, remove their log files and rerun the script if you want to.

## Expected runtime
- the main experiments take 8-12 hours to run
- the appendix experiments on their own take 12-16 hours to run

If you feel like your PC needs more computation time to achieve comparable results to what we present in our paper you can adjust the `timeout_multiplier` variable on top of the `experiments/experiments.py` file.

## Requirements
- Python3.8 or higher
- pdflatex (`sudo apt-get install texlive`) - we use this to generate the PDFs for the convinience, if you want to use different way we also provide the .tex source files so you can convert the tables/graphs yourself

## Orinal logs
The folder `experiments/original-results` contains our log files. The subfolders `models-info`, `q1`, `q2`, `q3`, `appendix` contain the log files generated from the tools. The subfolder `results` contains the PDFs generated from these logs.

## Experiments list
We provide a quick overview of the experiments here

#### Table 2 - models-info
This experiment provides the information about all of the considered models like their number of states/actions/observation and computed over-approximation using Storm

#### Table 3 - Q1
Experiment showing the potential improvements we can achieve by using FSCs generated by inductive synthesis in Storm

#### Table 3 - Q2
Experiment showing the improvements of PAYNT when we use policies computed by Storm to steer the inductive synthesis

#### Table 4 - size of FSCs (part of Q3 experiments)
We show the difference in value/size of the FSCs produced by SAYNT

#### Figure 4 - Q3
In this experiment we compare SAYNT/PAYNT/Storm on how they find FSCs over time. The results of this experiment are presented as a series of graphs each one for each model.

#### Appendix
In the appendix we provide more values we achieved when comparing SAYNT/PAYNT/Storm. The table in appendix provides information about the best quality FSC, when it was found and it's size within a 15 minute time limit.